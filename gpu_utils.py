import streamlit as st
import torch
import subprocess
import re
from typing import Tuple, Optional

class GPUDetector:
    def __init__(self):
        self.gpu_info = None
        self.vram_gb = 0
        self.device = "cpu"
        self.optimal_model = "base"
    
    def detect_gpu(self) -> Tuple[bool, str, float]:
        """GPU ê°ì§€ ë° VRAM í™•ì¸"""
        try:
            # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not torch.cuda.is_available():
                st.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PyTorchê°€ CUDAë¥¼ ì§€ì›í•˜ì§€ ì•Šê±°ë‚˜ GPU ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False, "CPU", 0.0
            
            # GPU ì •ë³´ ìˆ˜ì§‘
            gpu_count = torch.cuda.device_count()
            if gpu_count == 0:
                st.warning("CUDA ë””ë°”ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False, "CPU", 0.0
            
            # ì²« ë²ˆì§¸ GPU ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            gpu_name = torch.cuda.get_device_name(0)
            
            # nvidia-smië¡œ VRAM ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            vram_gb = self._get_vram_from_nvidia_smi()
            
            if vram_gb == 0:
                # nvidia-smi ì‹¤íŒ¨ ì‹œ PyTorchë¡œ ì¶”ì •
                vram_gb = self._estimate_vram_from_torch()
            
            # GTX 1650 íŠ¹ë³„ ì²˜ë¦¬ (4GB VRAM)
            if "GTX 1650" in gpu_name or "1650" in gpu_name or "GeForce GTX 1650" in gpu_name:
                vram_gb = 4.0  # GTX 1650ì€ 4GB VRAM
            
            # ê¸°íƒ€ GPUë“¤ì— ëŒ€í•œ VRAM ì¶”ì •
            if vram_gb < 2.0:  # VRAMì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì¶”ì •ê°’ ì‚¬ìš©
                if "RTX" in gpu_name:
                    if "3080" in gpu_name or "4070" in gpu_name:
                        vram_gb = 10.0
                    elif "3060" in gpu_name:
                        vram_gb = 8.0
                    elif "2060" in gpu_name:
                        vram_gb = 6.0
                elif "GTX" in gpu_name:
                    if "1660" in gpu_name:
                        vram_gb = 6.0
                    elif "1650" in gpu_name:
                        vram_gb = 4.0
                    elif "1060" in gpu_name:
                        vram_gb = 6.0
            
            self.gpu_info = {
                "name": gpu_name,
                "count": gpu_count,
                "vram_gb": vram_gb
            }
            
            return True, gpu_name, vram_gb
            
        except Exception as e:
            st.warning(f"GPU ê°ì§€ ì‹¤íŒ¨: {str(e)}")
            st.info("CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            return False, "CPU", 0.0
    
    def _get_vram_from_nvidia_smi(self) -> float:
        """nvidia-smië¡œ VRAM ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=memory.total", "--format=csv,noheader,nounits"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                vram_mb = int(result.stdout.strip().split('\n')[0])
                return vram_mb / 1024  # MBë¥¼ GBë¡œ ë³€í™˜
            return 0.0
            
        except Exception:
            return 0.0
    
    def _estimate_vram_from_torch(self) -> float:
        """PyTorchë¡œ VRAM ì¶”ì •"""
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            total_memory = torch.cuda.get_device_properties(0).total_memory
            return total_memory / (1024**3)  # bytesë¥¼ GBë¡œ ë³€í™˜
        except Exception:
            return 0.0
    
    def select_optimal_whisper_model(self, vram_gb: float) -> str:
        """VRAM ìš©ëŸ‰ì— ë”°ë¥¸ ìµœì  Whisper ëª¨ë¸ ì„ íƒ"""
        if vram_gb >= 10:
            return "large-v3"  # 10-12GB í•„ìš”
        elif vram_gb >= 7:
            return "medium"    # 7GB í•„ìš”
        elif vram_gb >= 6:
            return "small"     # 6GB ì´ìƒì—ì„œ small ì‚¬ìš©
        elif vram_gb >= 2:
            return "base"      # 2-5GBì—ì„œ base ì‚¬ìš© (4GB í¬í•¨)
        else:
            return "tiny"      # 1GB í•„ìš”
    
    def get_device_info(self) -> dict:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        gpu_available, gpu_name, vram_gb = self.detect_gpu()
        
        if gpu_available:
            optimal_model = self.select_optimal_whisper_model(vram_gb)
            device = "cuda"
        else:
            optimal_model = "base"  # CPUì—ì„œëŠ” base ëª¨ë¸ ì‚¬ìš©
            device = "cpu"
        
        return {
            "gpu_available": gpu_available,
            "gpu_name": gpu_name,
            "vram_gb": vram_gb,
            "device": device,
            "optimal_model": optimal_model
        }

def get_whisper_model_info() -> dict:
    """Whisper ëª¨ë¸ë³„ VRAM ìš”êµ¬ì‚¬í•­ ì •ë³´"""
    return {
        "tiny": {"vram_gb": 1, "description": "CPUë¡œë„ ì‹¤í–‰ ê°€ëŠ¥"},
        "base": {"vram_gb": 2, "description": "ë³´ê¸‰í˜• GPUë¡œë„ ì‹¤í–‰ ê°€ëŠ¥"},
        "small": {"vram_gb": 4, "description": "RTX 2060 ì´ìƒ ê¶Œì¥"},
        "medium": {"vram_gb": 7, "description": "RTX 3060 ì´ìƒ ê¶Œì¥"},
        "large-v3": {"vram_gb": 10, "description": "RTX 3080 ë˜ëŠ” RTX 4070 ì´ìƒ ê¶Œì¥"}
    }

def display_gpu_status():
    """GPU ìƒíƒœë¥¼ Streamlitì— í‘œì‹œ"""
    detector = GPUDetector()
    device_info = detector.get_device_info()
    
    if device_info["gpu_available"]:
        # ê°„ê²°í•œ í•œ ì¤„ í‘œì‹œ
        st.success(f"âœ… GPU ê°ì§€ë¨: {device_info['gpu_name']} ({device_info['vram_gb']:.1f}GB VRAM)")
        
        # ëª¨ë¸ë³„ VRAM ìš”êµ¬ì‚¬í•­ í‘œì‹œ (ê°„ê²°í•˜ê²Œ)
        model_info = get_whisper_model_info()
        st.write("**Whisper ëª¨ë¸ë³„ VRAM ìš”êµ¬ì‚¬í•­:**")
        for model, info in model_info.items():
            status = "âœ…" if device_info["vram_gb"] >= info["vram_gb"] else "âŒ"
            st.write(f"{status} **{model}**: {info['vram_gb']}GB - {info['description']}")
    else:
        st.warning("âš ï¸ GPUë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        st.info("ğŸ¯ CPU ëª¨ë“œ: BART ëª¨ë¸ ì‚¬ìš©")
    
    return device_info
