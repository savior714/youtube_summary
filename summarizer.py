import streamlit as st
from transformers import pipeline
import torch
import re
from gpu_utils import GPUDetector

class Summarizer:
    def __init__(self):
        self.models = {}
        self.load_models()
    
    def load_models(self):
        """LongT5 ì ì‘í˜• ëª¨ë¸ ë¡œë“œ (VRAMì— ë”°ë¼ ìµœì í™”)"""
        try:
            # GPU ì •ë³´ í™•ì¸
            detector = GPUDetector()
            device_info = detector.get_device_info()
            
            vram_gb = device_info["vram_gb"]
            device = device_info["device"]
            gpu_name = device_info["gpu_name"]
            
            st.info(f"ğŸ” GPU: {gpu_name} ({vram_gb} GB VRAM)")
            
            # VRAMë³„ LongT5 ë¡œë”© ì •ì±…
            if vram_gb >= 12:
                load_kwargs = {"device_map": "auto"}  # full precision
                chunk_size = 8000
                max_new_tokens = 800
                st.info("ğŸš€ ê³ ì„±ëŠ¥ GPU ê°ì§€ - Full Precision ëª¨ë“œ")
            elif vram_gb >= 8:
                load_kwargs = {"device_map": "auto", "load_in_8bit": True}
                chunk_size = 4000
                max_new_tokens = 600
                st.info("âš¡ ì¤‘ê³ ì„±ëŠ¥ GPU ê°ì§€ - 8bit ì–‘ìí™” ëª¨ë“œ")
            elif vram_gb >= 4:
                load_kwargs = {"device_map": "auto", "load_in_8bit": True}
                chunk_size = 2500
                max_new_tokens = 400
                st.info("ğŸ”§ ë³´ê¸‰í˜• GPU ê°ì§€ - 8bit ì–‘ìí™” ëª¨ë“œ")
            elif vram_gb >= 2:
                load_kwargs = {"device_map": "auto", "load_in_4bit": True}
                chunk_size = 1800
                max_new_tokens = 300
                st.info("ğŸ’¾ ì €ì‚¬ì–‘ GPU ê°ì§€ - 4bit ì–‘ìí™” ëª¨ë“œ")
            else:
                # CPU fallback
                load_kwargs = {"device_map": "cpu"}
                chunk_size = 1200
                max_new_tokens = 200
                st.info("ğŸ–¥ï¸ CPU ëª¨ë“œ - ìµœì†Œ ì„¤ì •")
            
            st.info(f"ğŸ§  ì„¤ì •: chunk_size={chunk_size}, max_new_tokens={max_new_tokens}")
            
            # LongT5 ëª¨ë¸ ë¡œë“œ
            st.info("ğŸ”„ LongT5 ëª¨ë¸ ë¡œë”© ì¤‘...")
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            model_name = "google/long-t5-tglobal-base"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **load_kwargs)
            model.eval()
            
            # ì„¤ì • ì €ì¥
            self.longt5_model = model
            self.longt5_tokenizer = tokenizer
            self.device = device
            self.chunk_size = chunk_size
            self.max_new_tokens = max_new_tokens
            
            st.success("âœ… LongT5 ì ì‘í˜• ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            st.error(f"LongT5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            st.info("ğŸ”„ BART ëª¨ë¸ë¡œ fallback...")
            # Fallback to BART models
            self._load_fallback_models()
    
    def _load_fallback_models(self):
        """BART ëª¨ë¸ fallback ë¡œë”©"""
        try:
            # í•œêµ­ì–´ìš© ëª¨ë¸ (KoBART)
            st.info("í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸ (KoBART) ë¡œë”© ì¤‘...")
            self.models['ko'] = pipeline(
                "summarization",
                model="gogamza/kobart-base-v2",
                tokenizer="gogamza/kobart-base-v2",
                device=-1,
                max_length=None
            )
            
            # ì˜ì–´ìš© ëª¨ë¸ (BART-large-cnn)
            st.info("ì˜ì–´ ìš”ì•½ ëª¨ë¸ (BART-large-cnn) ë¡œë”© ì¤‘...")
            self.models['en'] = pipeline(
                "summarization", 
                model="facebook/bart-large-cnn",
                tokenizer="facebook/bart-large-cnn",
                device=-1,
                max_length=None
            )
            
            # LongT5 ì‚¬ìš© ë¶ˆê°€ í”Œë˜ê·¸ ì„¤ì •
            self.longt5_model = None
            self.longt5_tokenizer = None
            
        except Exception as e:
            st.error(f"Fallback ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    def summarize_text(self, text, language='en', max_length=None, min_length=None):
        """LongT5 ì ì‘í˜• í…ìŠ¤íŠ¸ ìš”ì•½"""
        if not text.strip():
            return "ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = self.preprocess_text(text)
            st.info(f"ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
            
            # LongT5 ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            if hasattr(self, 'longt5_model') and self.longt5_model is not None:
                st.info("ğŸš€ LongT5 ì ì‘í˜• ìš”ì•½ ì‹œì‘...")
                return self._summarize_with_longt5(text, language)
            else:
                st.info("ğŸ”„ BART ëª¨ë¸ë¡œ ìš”ì•½...")
                return self._summarize_with_bart(text, language)
                
        except Exception as e:
            st.error(f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
    
    def _summarize_with_longt5(self, text, language):
        """LongT5 ì ì‘í˜• ìš”ì•½"""
        import torch
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if language == 'ko':
            prompt_prefix = "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìƒì„¸í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n\n"
        else:
            prompt_prefix = "Please provide a detailed and comprehensive summary of the following text. Include specific details and key points:\n\n"
        
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]
        st.info(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨ (ì²­í¬ í¬ê¸°: {self.chunk_size}ì)")
        
        chunk_summaries = []
        progress_text = st.empty()
        
        # ê° ì²­í¬ ìš”ì•½
        for i, chunk in enumerate(chunks):
            progress_text.info(f"ğŸ”„ ì²­í¬ ìš”ì•½ ì§„í–‰ ì¤‘: {i+1}/{len(chunks)}")
            
            try:
                # í”„ë¡¬í”„íŠ¸ì™€ ì²­í¬ ê²°í•©
                prompt_text = prompt_prefix + chunk
                
                # í† í¬ë‚˜ì´ì§•
                inputs = self.longt5_tokenizer(
                    prompt_text, 
                    return_tensors="pt", 
                    truncation=True, 
                    max_length=4096
                ).to(self.device)
                
                # ìš”ì•½ ìƒì„± (ì¼ê´€ì„±ì„ ìœ„í•´ deterministic ì„¤ì •)
                with torch.no_grad():
                    output = self.longt5_model.generate(
                        **inputs,
                        max_new_tokens=self.max_new_tokens,
                        no_repeat_ngram_size=3,
                        num_beams=4,  # ë¹” ì„œì¹˜ë¡œ ë” ì•ˆì •ì ì¸ ê²°ê³¼
                        early_stopping=True,
                        do_sample=False,  # ìƒ˜í”Œë§ ë¹„í™œì„±í™”ë¡œ ì¼ê´€ì„± í™•ë³´
                        temperature=1.0  # do_sample=Falseì¼ ë•ŒëŠ” ë¬´ì‹œë¨
                    )
                
                # ê²°ê³¼ ë””ì½”ë”©
                summary = self.longt5_tokenizer.decode(output[0], skip_special_tokens=True)
                chunk_summaries.append(summary)
                
                # VRAM ì •ë¦¬
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                st.warning(f"ì²­í¬ {i+1} ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì²­í¬ì˜ ì¼ë¶€ë¥¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                fallback_summary = chunk[:200] + "..." if len(chunk) > 200 else chunk
                chunk_summaries.append(fallback_summary)
        
        progress_text.success(f"âœ… ì²­í¬ ìš”ì•½ ì™„ë£Œ: {len(chunk_summaries)}ê°œ ì²­í¬ ì²˜ë¦¬ë¨")
        
        # ìµœì¢… ìš”ì•½ ìƒì„±
        if len(chunk_summaries) > 1:
            st.info("ğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
            combined_summaries = " ".join(chunk_summaries)
            
            try:
                # ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸
                if language == 'ko':
                    final_prompt = f"ë‹¤ìŒ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ ì²´ê³„ì ì´ê³  ìƒì„¸í•œ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ ëª¨ë‘ í¬í•¨í•´ì£¼ì„¸ìš”:\n\n{combined_summaries}"
                else:
                    final_prompt = f"Please create a comprehensive and detailed final summary by synthesizing the following content. Include all key points and specific details:\n\n{combined_summaries}"
                
                final_inputs = self.longt5_tokenizer(
                    final_prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.device)
                
                final_output = self.longt5_model.generate(
                    **final_inputs,
                    max_new_tokens=self.max_new_tokens,
                    no_repeat_ngram_size=3,
                    num_beams=4,  # ë¹” ì„œì¹˜ë¡œ ë” ì•ˆì •ì ì¸ ê²°ê³¼
                    early_stopping=True,
                    do_sample=False,  # ìƒ˜í”Œë§ ë¹„í™œì„±í™”ë¡œ ì¼ê´€ì„± í™•ë³´
                    temperature=1.0  # do_sample=Falseì¼ ë•ŒëŠ” ë¬´ì‹œë¨
                )
                
                final_summary = self.longt5_tokenizer.decode(final_output[0], skip_special_tokens=True)
                
                # VRAM ì •ë¦¬
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                
                return self._postprocess_summary(final_summary, language)
                
            except Exception as e:
                st.warning(f"ìµœì¢… ìš”ì•½ ì‹¤íŒ¨, ì²­í¬ ìš”ì•½ ê²°í•©: {str(e)}")
                return self._postprocess_summary(combined_summaries, language)
        else:
            return self._postprocess_summary(chunk_summaries[0], language)
    
    def _summarize_with_bart(self, text, language):
        """BART ëª¨ë¸ fallback ìš”ì•½"""
        # ê¸°ì¡´ BART ë¡œì§ ì‚¬ìš©
        if len(text) > 2000:
            st.info("ğŸ”„ ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ - ì²­í¬ ë‹¨ìœ„ë¡œ ìš”ì•½ ì¤‘...")
            summary = self._summarize_long_text(text, language)
        else:
            st.info("ğŸ”„ ì „ì²´ í…ìŠ¤íŠ¸ ìš”ì•½ ì¤‘...")
            summary = self._summarize_short_text(text, language)
        
        return self._postprocess_summary(summary, language)
    
    def _summarize_short_text(self, text, language):
        """ì§§ì€ í…ìŠ¤íŠ¸ ìš”ì•½"""
        # í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ìš”ì•½ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        if language == 'ko':
            prompt_text = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìƒì„¸í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n\n{text}"
        else:
            prompt_text = f"Please provide a detailed and comprehensive summary of the following text. Include specific details and key points:\n\n{text}"
        
        kwargs = {
            'do_sample': False,
            'truncation': True,  # í† í° ê¸¸ì´ ì´ˆê³¼ì‹œ ìë™ ìë¥´ê¸°
            'max_new_tokens': 500,  # ë” ê¸´ ìš”ì•½ì„ ìœ„í•´ ì¦ê°€
            'min_length': 100     # ë” ê¸´ ìµœì†Œ ê¸¸ì´
        }
        
        summary = self.models[language](prompt_text, **kwargs)
        return summary[0]['summary_text']
    
    def _summarize_long_text(self, text, language, recursion_depth=0):
        """ê¸´ í…ìŠ¤íŠ¸ ì²­í¬ ë‹¨ìœ„ ìš”ì•½ (ì¬ê·€ ê¹Šì´ ì œí•œ)"""
        # ì¬ê·€ ê¹Šì´ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if recursion_depth >= 3:
            st.warning("âš ï¸ ì¬ê·€ ê¹Šì´ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return text[:1000] + "..." if len(text) > 1000 else text
        
        # í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•œ í¬ê¸°ë¡œ ë¶„í•  (í† í° ê¸¸ì´ ê³ ë ¤)
        chunks = self._split_text_safely(text, max_chars=800)  # 800ìë¡œ ì¦ê°€í•˜ì—¬ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
        
        st.info(f"ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")
        
        # ê° ì²­í¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ìš”ì•½ (í•œì¤„ ì§„í–‰ë¥  í‘œì‹œ)
        chunk_summaries = []
        progress_text = st.empty()  # í•œì¤„ë¡œ ì—…ë°ì´íŠ¸ë˜ëŠ” ì§„í–‰ë¥  í‘œì‹œ
        
        for i, chunk in enumerate(chunks):
            if not chunk.strip():
                continue
                
            # í•œì¤„ë¡œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_text.info(f"ğŸ”„ ì²­í¬ ìš”ì•½ ì§„í–‰ ì¤‘: {i+1}/{len(chunks)}")
            
            try:
                # í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì²­í¬ ìš”ì•½
                if language == 'ko':
                    prompt_chunk = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸íˆ ìš”ì•½í•´ì£¼ì„¸ìš”. êµ¬ì²´ì ì¸ ì •ë³´ì™€ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n\n{chunk}"
                else:
                    prompt_chunk = f"Summarize the key points of the following text in detail, including specific information:\n\n{chunk}"
                
                # ì•ˆì „í•œ ê¸¸ì´ë¡œ ìš”ì•½ (í† í° ê¸¸ì´ ê³ ë ¤)
                summary = self.models[language](
                    prompt_chunk, 
                    do_sample=False, 
                    truncation=True,  # í† í° ê¸¸ì´ ì´ˆê³¼ì‹œ ìë™ ìë¥´ê¸°
                    max_new_tokens=300,  # ë” ê¸´ ì²­í¬ ìš”ì•½ì„ ìœ„í•´ ì¦ê°€
                    min_length=80     # ë” ê¸´ ìµœì†Œ ê¸¸ì´ë¡œ ìƒì„¸í•œ ìš”ì•½
                )
                chunk_summaries.append(summary[0]['summary_text'])
            except Exception as e:
                st.warning(f"ì²­í¬ {i+1} ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì²­í¬ì˜ ì¼ë¶€ë¥¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                fallback_summary = chunk[:150] + "..." if len(chunk) > 150 else chunk
                chunk_summaries.append(fallback_summary)
        
        # ì§„í–‰ë¥  í‘œì‹œ ì™„ë£Œ
        progress_text.success(f"âœ… ì²­í¬ ìš”ì•½ ì™„ë£Œ: {len(chunk_summaries)}ê°œ ì²­í¬ ì²˜ë¦¬ë¨")
        
        # ì²­í¬ ìš”ì•½ë“¤ì„ ê²°í•©
        combined_summaries = ' '.join(chunk_summaries)
        
        # ê²°í•©ëœ í…ìŠ¤íŠ¸ê°€ ì—¬ì „íˆ ê¸¸ë©´ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬ (ë” ì—„ê²©í•œ ì¡°ê±´)
        if len(combined_summaries) > 2000:  # 1000ì—ì„œ 2000ìœ¼ë¡œ ì¦ê°€
            st.info("ğŸ”„ ê²°í•©ëœ í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ ì¬ê·€ ìš”ì•½ ì§„í–‰...")
            return self._summarize_long_text(combined_summaries, language, recursion_depth + 1)
        else:
            # ìµœì¢… ìš”ì•½ ìƒì„± (ì•ˆì „í•œ ê¸¸ì´ë¡œ)
            st.info("ğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
            try:
                # ìµœì¢… ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
                if language == 'ko':
                    final_prompt = f"ë‹¤ìŒ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ ì²´ê³„ì ì´ê³  ìƒì„¸í•œ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ ëª¨ë‘ í¬í•¨í•´ì£¼ì„¸ìš”:\n\n{combined_summaries}"
                else:
                    final_prompt = f"Please create a comprehensive and detailed final summary by synthesizing the following content. Include all key points and specific details:\n\n{combined_summaries}"
                
                final_summary = self.models[language](
                    final_prompt, 
                    do_sample=False, 
                    truncation=True,  # í† í° ê¸¸ì´ ì´ˆê³¼ì‹œ ìë™ ìë¥´ê¸°
                    max_new_tokens=800,  # ë” í° ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìƒì„¸í•œ ìš”ì•½
                    min_length=300    # ë” í° ìµœì†Œ ê¸¸ì´ë¡œ ìƒì„¸í•œ ìš”ì•½ ë³´ì¥
                )
                return final_summary[0]['summary_text']
            except Exception as e:
                st.warning(f"ìµœì¢… ìš”ì•½ ì‹¤íŒ¨, ì²­í¬ ìš”ì•½ ê²°í•©: {str(e)}")
                return combined_summaries
    
    def _split_text_safely(self, text, max_chars=800):
        """í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•œ í¬ê¸°ë¡œ ë¶„í•  (í† í° ê¸¸ì´ ê³ ë ¤)"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = text.split('. ')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ í™•ì¸
            if current_length + sentence_length > max_chars and current_chunk:
                # í˜„ì¬ ì²­í¬ë¥¼ ì™„ë£Œí•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                chunks.append('. '.join(current_chunk) + '.')
                current_chunk = [sentence]
                current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length + 2  # '. ' ê¸¸ì´ ì¶”ê°€
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append('. '.join(current_chunk))
        
        return chunks
    
    def _postprocess_summary(self, summary, language):
        """ìš”ì•½ í›„ì²˜ë¦¬ - êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ê°œì„ """
        if not summary or len(summary.strip()) < 50:
            return summary
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = summary.split('. ')
        if len(sentences) < 3:
            return summary
        
        # êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
        if language == 'ko':
            structured_summary = "## ì£¼ìš” ë‚´ìš©\n\n"
        else:
            structured_summary = "## Key Points\n\n"
        
        # í•µì‹¬ ë¬¸ì¥ë“¤ì„ ì„ ë³„í•˜ì—¬ êµ¬ì¡°í™”
        key_sentences = []
        for i, sentence in enumerate(sentences):
            if sentence.strip() and len(sentence.strip()) > 20:
                # ë¬¸ì¥ ì•ì— ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ í¬ì¸íŠ¸ ì¶”ê°€
                if language == 'ko':
                    key_sentences.append(f"â€¢ {sentence.strip()}")
                else:
                    key_sentences.append(f"â€¢ {sentence.strip()}")
        
        # ìµœëŒ€ 8-10ê°œì˜ í•µì‹¬ ë¬¸ì¥ë§Œ ì„ íƒ
        selected_sentences = key_sentences[:min(10, len(key_sentences))]
        structured_summary += "\n".join(selected_sentences)
        
        # ë§ˆì§€ë§‰ì— ì „ì²´ ìš”ì•½ ì¶”ê°€
        if language == 'ko':
            structured_summary += "\n\n## ì „ì²´ ìš”ì•½\n\n" + summary
        else:
            structured_summary += "\n\n## Overall Summary\n\n" + summary
        
        return structured_summary
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?]', ' ', text)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ì—°ì†ëœ ê°™ì€ ë¬¸ì ì œê±° (ì˜ˆ: aaaaa -> a)
        text = re.sub(r'(.)\1{3,}', r'\1', text)
        return text.strip()
    
    def split_text(self, text, max_length):
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            if current_length + len(word) + 1 > max_length:
                if current_chunk:
                    chunks.append(" ".join(current_chunk))
                    current_chunk = [word]
                    current_length = len(word)
            else:
                current_chunk.append(word)
                current_length += len(word) + 1
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
