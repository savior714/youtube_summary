import streamlit as st
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from typing import List, Dict, Tuple, Any
from summarizer import Summarizer
from api_summarizer import APISummarizer

class HybridSummarizer:
    def __init__(self):
        self.local_summarizer = Summarizer()
        self.api_summarizer = APISummarizer()
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
    
    def split_into_paragraphs(self, text: str, min_length: int = 200) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• """
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        paragraphs = []
        current_paragraph = []
        current_length = 0
        
        for sentence in sentences:
            if current_length + len(sentence) > min_length and current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = [sentence]
                current_length = len(sentence)
            else:
                current_paragraph.append(sentence)
                current_length += len(sentence)
        
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        return paragraphs
    
    def generate_embeddings(self, paragraphs: List[str]) -> np.ndarray:
        """ë¬¸ë‹¨ë³„ ì„ë² ë”© ìƒì„±"""
        try:
            # TF-IDF ë²¡í„°í™”
            tfidf_matrix = self.vectorizer.fit_transform(paragraphs)
            return tfidf_matrix.toarray()
        except Exception as e:
            st.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return np.array([])
    
    def find_important_paragraphs(self, paragraphs: List[str], embeddings: np.ndarray, 
                                top_k: int = 5) -> List[Tuple[int, str, float]]:
        """ì¤‘ìš”í•œ ë¬¸ë‹¨ ì°¾ê¸°"""
        if len(paragraphs) == 0 or embeddings.size == 0:
            return []
        
        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
            avg_embedding = np.mean(embeddings, axis=0).reshape(1, -1)
            
            # ê° ë¬¸ë‹¨ê³¼ í‰ê·  ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(embeddings, avg_embedding).flatten()
            
            # ìƒìœ„ kê°œ ë¬¸ë‹¨ ì„ íƒ
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            important_paragraphs = []
            for idx in top_indices:
                if idx < len(paragraphs):
                    important_paragraphs.append((idx, paragraphs[idx], similarities[idx]))
            
            return important_paragraphs
            
        except Exception as e:
            st.error(f"ì¤‘ìš” ë¬¸ë‹¨ ì°¾ê¸° ì‹¤íŒ¨: {str(e)}")
            return []
    
    def summarize_paragraphs(self, paragraphs: List[Tuple[int, str, float]], 
                           language: str = "ko", max_length: int = 150) -> List[str]:
        """ë¬¸ë‹¨ë³„ ìš”ì•½"""
        summaries = []
        
        for i, (idx, paragraph, score) in enumerate(paragraphs):
            st.info(f"ë¬¸ë‹¨ {i+1}/{len(paragraphs)} ìš”ì•½ ì¤‘... (ì¤‘ìš”ë„: {score:.3f})")
            
            # ë¬¸ë‹¨ ìš”ì•½
            summary = self.local_summarizer.summarize_text(
                paragraph, 
                language=language, 
                max_length=max_length // len(paragraphs),  # ì „ì²´ ê¸¸ì´ë¥¼ ë¬¸ë‹¨ ìˆ˜ë¡œ ë‚˜ëˆ”
                min_length=20
            )
            summaries.append(summary)
        
        return summaries
    
    def create_meta_summary(self, chunk_summaries: List[str], 
                          language: str = "ko", max_length: int = 150) -> str:
        """ì²­í¬ ìš”ì•½ë“¤ì„ í†µí•œ ë©”íƒ€ ìš”ì•½ ìƒì„±"""
        combined_text = " ".join(chunk_summaries)
        
        # ë©”íƒ€ ìš”ì•½ ìƒì„±
        meta_summary = self.local_summarizer.summarize_text(
            combined_text,
            language=language,
            max_length=max_length,
            min_length=50
        )
        
        return meta_summary
    
    def hybrid_summarize(self, text: str, language: str = "ko", max_length: int = None,
                        use_api: bool = False, api_provider: str = None, api_key: str = None) -> Dict[str, Any]:
        """Hybrid ìš”ì•½ ì‹¤í–‰"""
        st.info("ğŸ”„ Hybrid ìš”ì•½ ì‹œì‘...")
        
        # max_lengthê°€ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if max_length is None:
            max_length = 200  # ê¸°ë³¸ ìš”ì•½ ê¸¸ì´
        
        # 1ë‹¨ê³„: ë¬¸ë‹¨ ë¶„í• 
        st.info("1ï¸âƒ£ ë¬¸ë‹¨ ë¶„í•  ì¤‘...")
        paragraphs = self.split_into_paragraphs(text)
        st.info(f"ì´ {len(paragraphs)}ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• ë¨")
        
        if len(paragraphs) == 0:
            return {"error": "ë¬¸ë‹¨ì„ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        # 2ë‹¨ê³„: ì„ë² ë”© ìƒì„±
        st.info("2ï¸âƒ£ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.generate_embeddings(paragraphs)
        
        if embeddings.size == 0:
            return {"error": "ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
        
        # 3ë‹¨ê³„: ì¤‘ìš”í•œ ë¬¸ë‹¨ ì„ íƒ
        st.info("3ï¸âƒ£ ì¤‘ìš”í•œ ë¬¸ë‹¨ ì„ íƒ ì¤‘...")
        important_paragraphs = self.find_important_paragraphs(paragraphs, embeddings, top_k=min(5, len(paragraphs)))
        
        if not important_paragraphs:
            return {"error": "ì¤‘ìš”í•œ ë¬¸ë‹¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        st.info(f"ìƒìœ„ {len(important_paragraphs)}ê°œ ë¬¸ë‹¨ ì„ íƒë¨")
        
        # 4ë‹¨ê³„: ë¬¸ë‹¨ë³„ ìš”ì•½
        st.info("4ï¸âƒ£ ë¬¸ë‹¨ë³„ ìš”ì•½ ì¤‘...")
        chunk_summaries = self.summarize_paragraphs(important_paragraphs, language, max_length)
        
        # 5ë‹¨ê³„: ë©”íƒ€ ìš”ì•½ ìƒì„±
        st.info("5ï¸âƒ£ ë©”íƒ€ ìš”ì•½ ìƒì„± ì¤‘...")
        meta_summary = self.create_meta_summary(chunk_summaries, language, max_length)
        
        # API ì‚¬ìš© ì‹œ ì¶”ê°€ ìš”ì•½
        if use_api and api_provider and api_key:
            st.info("6ï¸âƒ£ APIë¥¼ í†µí•œ ìµœì¢… ìš”ì•½ ì¤‘...")
            api_summary = self.api_summarizer.summarize_text(
                meta_summary, 
                api_provider, 
                api_key, 
                language, 
                max_length
            )
            final_summary = api_summary
        else:
            final_summary = meta_summary
        
        return {
            "summary": final_summary,
            "chunk_summaries": chunk_summaries,
            "important_paragraphs": important_paragraphs,
            "paragraph_count": len(paragraphs),
            "selected_count": len(important_paragraphs)
        }
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?]', ' ', text)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ì—°ì†ëœ ê°™ì€ ë¬¸ì ì œê±°
        text = re.sub(r'(.)\1{3,}', r'\1', text)
        return text.strip()
